"""
Extended validation tests for nanorod agglomerate simulation.

Validates our particle-cluster (DLA) Brownian collision simulation against
the results in Abomailek et al. (Small 2025, DOI: 10.1002/smll.202409673).

IMPORTANT — what the paper's D_f = 1.80 actually measures:

    The paper's D_f = 1.80 comes from EXPERIMENTAL SEM data of real gas-phase
    agglomerates that form through diffusion-limited cluster-cluster aggregation
    (DLCA). It is measured using the projected area (PA) method on 2D SEM images:
    R_g from the binarized projection, N estimated from area ratios.

    Our simulation (and the paper's simulation) uses particle-cluster aggregation
    (DLA), where individual rods join a growing cluster one at a time. The paper
    explicitly states: "These models generated by simulation presently do not
    follow a realistic dynamic process of formation; they are generated by
    aggregation of individual nanowires onto a single agglomerate and ignore
    cluster-cluster collisions" (page 6).

    For reference, well-known D_f values for different aggregation regimes:
      - Spherical DLCA (cluster-cluster):  D_f ~ 1.78
      - Spherical DLA  (particle-cluster): D_f ~ 2.5
      - Rod DLA        (particle-cluster): D_f ~ 2.0 (our simulation)

    The paper uses its simulation ONLY for:
      1. D_f,BC (2D box-counting from projections) — our 1.58 matches Fig 7B
      2. Validating b-normalization (Fig 8A) — our collapse test passes
      3. R_agg growth comparison (Fig 8B)

    It never reports a 3D mass-radius D_f from the simulation.

Run with:
    pytest -m slow -v -s          # extended tests only, with diagnostics
    pytest -m "not slow" -v       # fast CI tests only
    pytest -v                     # everything

Expected runtime: ~2-4 min with numba, ~5-10 min without.
"""

import numpy as np
import pytest

from agglomerate import generate_agglomerate
from analyze_batch_paper_method import calculate_fractal_dimension_paper_method


# ---------------------------------------------------------------------------
# Shared constants
# ---------------------------------------------------------------------------

LENGTH = 1000       # nm — aspect ratio 67 (close to paper's 73)
DIAMETER = 15       # nm
SEEDS = [10, 20, 30, 40, 50]
N_VALUES = [10, 20, 50, 100, 200]


# ---------------------------------------------------------------------------
# Helpers (shared across test classes)
# ---------------------------------------------------------------------------

def radius_of_gyration(positions):
    """Compute R_g = sqrt(1/N * sum |r_i - r_com|^2)."""
    com = positions.mean(axis=0)
    return np.sqrt(np.mean(np.sum((positions - com) ** 2, axis=1)))


def volume_equivalent_radius(length, diameter):
    """Compute b = (3/4 * R_NW^2 * L)^(1/3), the volume-equivalent sphere radius (Eq 5)."""
    return (0.75 * (diameter / 2) ** 2 * length) ** (1 / 3)


def _generate_and_get_positions(n, length, diameter, seed):
    """Generate an agglomerate and return (agglomerate, positions array)."""
    agg = generate_agglomerate(n, length, diameter, seed=seed, verbose=False)
    return agg, np.array([rod.center for rod in agg])


def _fit_paper_direction(n_values, mean_rg, b):
    """
    Fit in the paper's direction: log(N) = D_f * log(R_g/b) + log(k'_g).

    This matches Equation 4 of the paper: N = k'_0 * (R_g / b)^D_f.

    Returns (D_f, k_g_prime, R_squared).
    """
    log_n = np.log(np.array(n_values, dtype=float))
    log_rg_norm = np.log(np.array(mean_rg) / b)

    # linear regression: log(N) = slope * log(R_g/b) + intercept
    coeffs = np.polyfit(log_rg_norm, log_n, 1)
    slope, intercept = coeffs[0], coeffs[1]

    y_pred = np.polyval(coeffs, log_rg_norm)
    ss_res = np.sum((log_n - y_pred) ** 2)
    ss_tot = np.sum((log_n - np.mean(log_n)) ** 2)
    r_squared = 1 - ss_res / ss_tot

    d_f = slope
    k_g_prime = np.exp(intercept)

    return d_f, k_g_prime, r_squared


# ---------------------------------------------------------------------------
# Test classes
# ---------------------------------------------------------------------------

@pytest.mark.slow
class TestMassRadiusScaling:
    """
    3D mass-radius fractal dimension from particle-cluster DLA.

    Our simulation produces D_f ~ 2.0 from the 3D center-of-mass R_g.
    This is physically expected for particle-cluster aggregation of
    high-aspect-ratio rods (spherical DLA gives D_f ~ 2.5; rod
    anisotropy reduces it).

    The paper's D_f = 1.80 is from experimental DLCA data and is not
    directly comparable to our 3D particle-cluster simulation.
    """

    @pytest.fixture(scope="class")
    def scaling_data(self):
        """Generate agglomerates and compute mean R_g for each N."""
        b = volume_equivalent_radius(LENGTH, DIAMETER)
        results = {}

        for n in N_VALUES:
            rgs = []
            for seed in SEEDS:
                _, positions = _generate_and_get_positions(n, LENGTH, DIAMETER, seed)
                rgs.append(radius_of_gyration(positions))
            results[n] = {
                "mean_rg": np.mean(rgs),
                "std_rg": np.std(rgs),
                "rgs": rgs,
            }

        return results, b

    def test_power_law_fit_quality(self, scaling_data):
        """log-log fit of N vs R_g/b should have R^2 > 0.92."""
        results, b = scaling_data
        mean_rg = [results[n]["mean_rg"] for n in N_VALUES]

        d_f, k_g_prime, r_squared = _fit_paper_direction(N_VALUES, mean_rg, b)

        print(f"\n  Mass-radius scaling (3D, particle-cluster DLA):")
        print(f"  D_f = {d_f:.3f}, k'_g = {k_g_prime:.4f}, R^2 = {r_squared:.4f}")
        print(f"  {'N':>5s}  {'R_g':>10s}  {'R_g/b':>10s}  {'std':>10s}")
        for n in N_VALUES:
            r = results[n]
            print(f"  {n:5d}  {r['mean_rg']:10.1f}  {r['mean_rg']/b:10.3f}  {r['std_rg']:10.1f}")

        assert r_squared > 0.92, f"Power-law fit R^2 = {r_squared:.3f}, expected > 0.92"

    def test_fractal_dimension_range(self, scaling_data):
        """D_f should be in the particle-cluster DLA range for rods: (1.7, 2.3).

        Spherical DLA gives D_f ~ 2.5; rod anisotropy lowers this.
        Our simulation consistently produces D_f ~ 2.0.
        """
        results, b = scaling_data
        mean_rg = [results[n]["mean_rg"] for n in N_VALUES]
        d_f, _, _ = _fit_paper_direction(N_VALUES, mean_rg, b)

        assert 1.7 < d_f < 2.3, f"D_f = {d_f:.2f}, expected in (1.7, 2.3)"

    def test_stability_across_n_range(self, scaling_data):
        """D_f should be stable when fitted over N<=100 vs N<=200.

        A stable D_f (within 0.15) across different N ranges confirms the
        power-law scaling is genuine and not a finite-size artifact.
        """
        results, b = scaling_data

        # Fit using only N <= 100
        n_small = [n for n in N_VALUES if n <= 100]
        rg_small = [results[n]["mean_rg"] for n in n_small]
        d_f_small, _, _ = _fit_paper_direction(n_small, rg_small, b)

        # Fit using all N <= 200
        rg_all = [results[n]["mean_rg"] for n in N_VALUES]
        d_f_all, _, _ = _fit_paper_direction(N_VALUES, rg_all, b)

        print(f"\n  Stability: D_f(N<=100) = {d_f_small:.3f}, D_f(N<=200) = {d_f_all:.3f}")
        print(f"  |difference| = {abs(d_f_all - d_f_small):.3f}")

        assert abs(d_f_all - d_f_small) < 0.15, (
            f"D_f should be stable across N ranges: "
            f"D_f(N<=100)={d_f_small:.3f}, D_f(N<=200)={d_f_all:.3f}, "
            f"difference={abs(d_f_all - d_f_small):.3f}"
        )

    def test_prefactor_self_consistency(self, scaling_data):
        """The fitted power law N = k'_g * (R_g/b)^D_f should predict N within 30%.

        k'_g is exponentially sensitive to D_f, so we test self-consistency
        (does the fit predict N accurately?) rather than comparing to the
        paper's k'_g = 2.04, which comes from experimental DLCA data.
        """
        results, b = scaling_data
        mean_rg = [results[n]["mean_rg"] for n in N_VALUES]
        d_f, k_g_prime, r_squared = _fit_paper_direction(N_VALUES, mean_rg, b)

        print(f"\n  Prefactor self-consistency:")
        print(f"  D_f = {d_f:.3f}, k'_g = {k_g_prime:.4f}")
        print(f"  {'N_true':>7s}  {'N_pred':>8s}  {'error':>8s}")
        max_error = 0
        for i, n in enumerate(N_VALUES):
            n_pred = k_g_prime * (mean_rg[i] / b) ** d_f
            error = abs(n_pred - n) / n
            max_error = max(max_error, error)
            print(f"  {n:7d}  {n_pred:8.1f}  {error:7.1%}")

        assert max_error < 0.30, (
            f"Power law should predict N within 30%, worst error = {max_error:.1%}"
        )


@pytest.mark.slow
class TestBoxCountingFractalDimension:
    """
    2D box-counting fractal dimension from projected agglomerates.

    This is the DIRECT comparison with the paper's simulation results
    (Figure 7B). The paper computes D_f,BC from 2D projections of simulated
    agglomerates using the same box-counting method we implement in
    analyze_batch_paper_method.py.

    The paper's Figure 7B shows D_f,BC increasing rapidly up to N ~ 100,
    then leveling off at ~ 1.6-1.7 for the simulation.
    """

    @pytest.fixture(scope="class")
    def box_counting_data(self):
        """Generate agglomerates and compute D_f,BC for each N."""
        results = {}

        for n in N_VALUES:
            dfs = []
            for seed in SEEDS:
                agg = generate_agglomerate(n, LENGTH, DIAMETER, seed=seed, verbose=False)
                np.random.seed(seed + 1000)  # separate seed for projection randomness
                mean_df, std_df, _ = calculate_fractal_dimension_paper_method(
                    agg, n_projections=3, resolution=1000
                )
                dfs.append(mean_df)
            results[n] = {
                "mean_df_bc": np.mean(dfs),
                "std_df_bc": np.std(dfs),
                "dfs": dfs,
            }

        return results

    def test_df_bc_increases_overall(self, box_counting_data):
        """D_f,BC should increase from N=10 to N=200 (paper Fig 7B trend)."""
        results = box_counting_data
        assert results[200]["mean_df_bc"] > results[10]["mean_df_bc"], (
            f"D_f,BC(N=200)={results[200]['mean_df_bc']:.3f} should exceed "
            f"D_f,BC(N=10)={results[10]['mean_df_bc']:.3f}"
        )

    def test_df_bc_range_at_large_n(self, box_counting_data):
        """D_f,BC(N=200) should be in [1.4, 1.9] (matching paper Fig 7B)."""
        results = box_counting_data

        print(f"\n  Box-counting fractal dimension (2D projections, paper method):")
        print(f"  {'N':>5s}  {'D_f,BC':>10s}  {'std':>10s}")
        for n in N_VALUES:
            r = results[n]
            print(f"  {n:5d}  {r['mean_df_bc']:10.3f}  {r['std_df_bc']:10.3f}")

        df_200 = results[200]["mean_df_bc"]
        assert 1.4 <= df_200 <= 1.9, (
            f"D_f,BC(N=200) = {df_200:.3f}, expected in [1.4, 1.9]"
        )

    def test_monotonic_trend(self, box_counting_data):
        """D_f,BC should show a broadly monotonic trend (pairwise checks)."""
        results = box_counting_data
        assert results[200]["mean_df_bc"] > results[50]["mean_df_bc"], (
            f"D_f,BC(N=200)={results[200]['mean_df_bc']:.3f} should exceed "
            f"D_f,BC(N=50)={results[50]['mean_df_bc']:.3f}"
        )
        assert results[100]["mean_df_bc"] > results[20]["mean_df_bc"], (
            f"D_f,BC(N=100)={results[100]['mean_df_bc']:.3f} should exceed "
            f"D_f,BC(N=20)={results[20]['mean_df_bc']:.3f}"
        )


@pytest.mark.slow
class TestNormalizationCollapse:
    """
    b-normalization collapse test (paper Fig 8A).

    Uses 3 geometries (L=500, 1000, 2000; all D=15) and coefficient of
    variation (CV) as the spread metric.

    The paper validates b = (3/4 * R_NW^2 * L)^(1/3) (Eq 5) as the correct
    normalization factor by showing that the y-intercept of log(N) vs log(R_g)
    scales linearly with b for simulated agglomerates of different rod lengths.
    This is a simulation-to-simulation comparison (no experimental data),
    so our results should match directly.
    """

    @pytest.fixture(scope="class")
    def collapse_data(self):
        """Generate agglomerates for 3 geometries and compute R_g and R_g/b."""
        geometries = [
            {"length": 500, "diameter": 15},
            {"length": 1000, "diameter": 15},
            {"length": 2000, "diameter": 15},
        ]
        n_values = [20, 50, 100, 200]

        data = {}
        for geom in geometries:
            L = geom["length"]
            D = geom["diameter"]
            b = volume_equivalent_radius(L, D)
            key = (L, D)
            data[key] = {"b": b, "rg": {}, "rg_norm": {}}

            for n in n_values:
                rgs = []
                for seed in SEEDS:
                    _, positions = _generate_and_get_positions(n, L, D, seed=seed)
                    rgs.append(radius_of_gyration(positions))
                mean_rg = np.mean(rgs)
                data[key]["rg"][n] = mean_rg
                data[key]["rg_norm"][n] = mean_rg / b

        return geometries, n_values, data

    def test_normalization_reduces_spread(self, collapse_data):
        """Normalizing R_g by b should reduce the CV across geometries."""
        geometries, n_values, data = collapse_data

        keys = [(g["length"], g["diameter"]) for g in geometries]

        raw_cvs = []
        norm_cvs = []

        for n in n_values:
            raw_vals = np.array([data[k]["rg"][n] for k in keys])
            norm_vals = np.array([data[k]["rg_norm"][n] for k in keys])

            raw_cvs.append(np.std(raw_vals) / np.mean(raw_vals))
            norm_cvs.append(np.std(norm_vals) / np.mean(norm_vals))

        mean_raw_cv = np.mean(raw_cvs)
        mean_norm_cv = np.mean(norm_cvs)

        print(f"\n  Normalization collapse (CV metric):")
        print(f"  {'N':>5s}  {'raw CV':>10s}  {'norm CV':>10s}")
        for i, n in enumerate(n_values):
            print(f"  {n:5d}  {raw_cvs[i]:10.4f}  {norm_cvs[i]:10.4f}")
        print(f"  Mean:  {mean_raw_cv:10.4f}  {mean_norm_cv:10.4f}")

        assert mean_norm_cv < mean_raw_cv, (
            f"Normalized CV ({mean_norm_cv:.4f}) should be less than "
            f"raw CV ({mean_raw_cv:.4f})"
        )

    def test_reduction_magnitude(self, collapse_data):
        """Normalization should reduce CV by at least 30%."""
        geometries, n_values, data = collapse_data

        keys = [(g["length"], g["diameter"]) for g in geometries]

        raw_cvs = []
        norm_cvs = []

        for n in n_values:
            raw_vals = np.array([data[k]["rg"][n] for k in keys])
            norm_vals = np.array([data[k]["rg_norm"][n] for k in keys])

            raw_cvs.append(np.std(raw_vals) / np.mean(raw_vals))
            norm_cvs.append(np.std(norm_vals) / np.mean(norm_vals))

        mean_raw_cv = np.mean(raw_cvs)
        mean_norm_cv = np.mean(norm_cvs)
        reduction = 1 - mean_norm_cv / mean_raw_cv

        print(f"\n  CV reduction: {reduction:.1%}")

        assert reduction > 0.30, (
            f"CV reduction = {reduction:.1%}, expected > 30%. "
            f"Raw CV = {mean_raw_cv:.4f}, Norm CV = {mean_norm_cv:.4f}"
        )
